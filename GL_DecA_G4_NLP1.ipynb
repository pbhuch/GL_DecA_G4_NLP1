{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GL_DecA_G4_NLP1.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pbhuch/GL_DecA_G4_NLP1/blob/main/GL_DecA_G4_NLP1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Vqh91aLDKOE"
      },
      "source": [
        "Great Learning : AIML Online Capstone -AUTOMATIC TICKET ASSIGNMENT\n",
        "\n",
        "DecA : Group 4: NLP 1\n",
        "\n",
        "Group Members :\n",
        "1. Priya Moily\n",
        "2. Priyanka Gupta\n",
        "3. Avinash Balani\n",
        "4. Priyank Bhuch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DOAo9YOEdQk"
      },
      "source": [
        "Importing relavant Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ATvXsfGZ8DiU"
      },
      "source": [
        "!pip install ftfy\n",
        "from time import time\n",
        "from PIL import Image\n",
        "from zipfile import ZipFile\n",
        "import os, sys, itertools, re\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from sklearn.model_selection import train_test_split\n",
        "import plotly as py\n",
        "import plotly.graph_objs as go\n",
        "import plotly.express as px\n",
        "from plotly.offline import init_notebook_mode, iplot, plot\n",
        "from sklearn.preprocessing import QuantileTransformer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score,f1_score,recall_score,precision_score, confusion_matrix, classification_report\n",
        "import sklearn.neighbors._base\n",
        "\n",
        "import imblearn\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.initializers import Constant\n",
        "from tensorflow.keras.layers import Flatten, Dense, Dropout, BatchNormalization, Activation, Conv2D, MaxPooling2D, Reshape, Embedding, LSTM,  TimeDistributed, Bidirectional, Lambda, Input, Add, GlobalMaxPool1D\n",
        "from tensorflow.keras import regularizers, optimizers\n",
        "from sklearn.metrics import r2_score\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier, KerasRegressor\n",
        "#import cv2\n",
        "from tensorflow.keras.applications.mobilenet import preprocess_input\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "# to define loss\n",
        "from tensorflow.keras.losses import binary_crossentropy\n",
        "from tensorflow.keras.backend import log, epsilon\n",
        "\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.datasets import imdb\n",
        "\n",
        "from itertools import islice\n",
        "\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "from ftfy import fix_encoding, fix_text, badness\n",
        "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
        "\n",
        "from sklearn.utils import resample\n",
        "\n",
        "import pickle, string\n",
        "\n",
        "import cufflinks as cf\n",
        "cf.go_offline()\n",
        "cf.set_config_file(offline=False, world_readable=True)\n",
        "\n",
        "import plotly as py\n",
        "import plotly.graph_objs as go\n",
        "from plotly.offline import download_plotlyjs,init_notebook_mode,plot,iplot\n",
        "\n",
        "import spacy\n",
        "\n",
        "# Suppress warnings\n",
        "import warnings; warnings.filterwarnings('ignore')\n",
        "\n",
        "SEED = 123                 # to be able to rerun the same NN\n",
        "np.random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "\n",
        "from IPython.display import display\n",
        "pd.options.display.max_columns = None\n",
        "pd.options.display.max_rows = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1V5dQobNvWh"
      },
      "source": [
        "pip install langdetect\n",
        "from langdetect import detect"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6VfBGuCNzay"
      },
      "source": [
        "from langdetect import detect\n",
        "import googletrans\n",
        "from googletrans import Translator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6ZAFKJoN4eF"
      },
      "source": [
        "!pip install goslate\n",
        "from goslate import Goslate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0VgAUvRROCJS"
      },
      "source": [
        "!pip install spacy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NiYT8_d5OeWt"
      },
      "source": [
        "# Load the dataset into a Pandas dataframe called \"dataset\" and check the head of the dataset\n",
        "dataset = pd.read_excel('sample_data/input_data.xlsx', )\n",
        "dataset.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "envtDeCYOzbh"
      },
      "source": [
        "dataset.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wkTBZmVIO1KM"
      },
      "source": [
        "dataset.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1pcGBkYEO1pG"
      },
      "source": [
        "dataset.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0wfmTtzqO3QF"
      },
      "source": [
        "# Find out the null value counts in each column\n",
        "dataset.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mzL4cTJ1PJU-"
      },
      "source": [
        "dataset[pd.isnull(dataset).any(axis=1)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQLbR5shPMk9"
      },
      "source": [
        "# NULL replacement\n",
        "dataset.fillna(str(), inplace=True)\n",
        "dataset.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYh5teM5PP7H"
      },
      "source": [
        "dataset[pd.isnull(dataset).any(axis=1)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uadwNHXpPULv"
      },
      "source": [
        "duplicate = dataset[dataset.duplicated()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ogha6GSQPXHF"
      },
      "source": [
        "duplicate.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Swrb0o_sPkmt"
      },
      "source": [
        "dataset1 = dataset[~dataset.duplicated()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P1ho3fs0PnLj"
      },
      "source": [
        "dataset1.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZtj5PmlPoeu"
      },
      "source": [
        "dataset1.insert(loc=4, \n",
        "              column='combined_description', \n",
        "              allow_duplicates=True, \n",
        "              value=list(dataset1['Short description'].str.strip() + ' ' + dataset1['Description'].str.strip()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPLdVudZUl-1"
      },
      "source": [
        "def fn_decode_to_ascii(df):\n",
        "  text = df.encode().decode('utf-8').encode('ascii', 'ignore')\n",
        "  return text.decode(\"utf-8\") \n",
        "\n",
        "dataset1['combined_description'] = dataset1['combined_description'].apply(fn_decode_to_ascii)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDoZ79gdRwGs"
      },
      "source": [
        "dataset1.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVQn_oZeRxl-"
      },
      "source": [
        "dataset1.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-RHo1a2Ry3F"
      },
      "source": [
        "dataset1.to_csv(\"sample_data/dataset_combined.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kPMTGvq4R1kH"
      },
      "source": [
        "# Write a function to apply to the dataset to detect Mojibakes\n",
        "def is_mojibake_impacted(text):\n",
        "    if not badness.sequence_weirdness(text):\n",
        "        # nothing weird, should be okay\n",
        "        return True\n",
        "    try:\n",
        "        text.encode('sloppy-windows-1252')\n",
        "    except UnicodeEncodeError:\n",
        "        # Not CP-1252 encodable, probably fine\n",
        "        return True\n",
        "    else:\n",
        "        # Encodable as CP-1252, Mojibake alert level high\n",
        "        return False\n",
        "    \n",
        "# Check the dataset for mojibake impact\n",
        "dataset1[~dataset1.iloc[:,:].applymap(is_mojibake_impacted).all(1)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_VVQiIqR5Ul"
      },
      "source": [
        "# Take an example of row# 8471 combined_description and fix it\n",
        "print('Garbled text: \\033[1m%s\\033[0m\\nFixed text: \\033[1m%s\\033[0m' % (dataset1['combined_description'][8471], \n",
        "                                                                        fix_text(dataset1['combined_description'][8471])))\n",
        "\n",
        "# List all mojibakes defined in ftfy library\n",
        "#print('\\nMojibake Symbol RegEx:\\n', badness.MOJIBAKE_SYMBOL_RE.pattern)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLxzLkinSE8O"
      },
      "source": [
        "dataset1['Short description']=dataset1['Short description'].apply(fix_text)\n",
        "dataset1['Description']=dataset1['Description'].apply(fix_text)\n",
        "dataset1['combined_description']=dataset1['combined_description'].apply(fix_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kEI6YXVcSIec"
      },
      "source": [
        "# Visualize row#8471\n",
        "dataset1.loc[8471,:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVGsNvgZjhn_"
      },
      "source": [
        "def fn_lan_detect(df):                                        \n",
        "   try:                                                          \n",
        "      return detect(df)                                      \n",
        "   except:                                                       \n",
        "      return 'no'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Auy0HdU5jRUg"
      },
      "source": [
        "dataset['Language'] = dataset['combined_description'].apply(fn_lan_detect)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gffWlIsLjRrH"
      },
      "source": [
        "x = dataset1[\"Language\"].value_counts()\n",
        "x=x.sort_index()\n",
        "plt.figure(figsize=(10,6))\n",
        "ax= sns.barplot(x.index, x.values, alpha=0.8)\n",
        "plt.title(\"Distribution of text by language\")\n",
        "plt.ylabel('number of records')\n",
        "plt.xlabel('Language')\n",
        "rects = ax.patches\n",
        "labels = x.values\n",
        "for rect, label in zip(rects, labels):\n",
        "    height = rect.get_height()\n",
        "    ax.text(rect.get_x() + rect.get_width()/2, height + 5, label, ha='center', va='bottom')\n",
        "plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SqY6vhaBSKT-"
      },
      "source": [
        "dataset1.to_csv('sample_data/dataset_mojibake_treated_LanguageDetected.csv', index=False, encoding='utf_8_sig')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6P8hQ_CZSrJz"
      },
      "source": [
        "email_regex = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1zQSfDpSMRc"
      },
      "source": [
        "def fn_remove_irrelaventWords(df,columnName):\n",
        "  for index in range(df.shape[0]):\n",
        "    df[columnName][index] = df[columnName][index].lower()                                       # to lower case \n",
        "    df[columnName][index] = re.sub(email_regex,\"\",df.loc[index,columnName])                     # remove email address\n",
        "    df[columnName][index] = re.sub(r'\\S*@\\S*\\s?', '', df.loc[index,columnName])                 # remove email address with appended text\n",
        "    df[columnName][index] = re.sub(r\"received from:\",' ',df.loc[index,columnName])              # remove unwanted text\n",
        "    df[columnName][index] = re.sub(r\"from:\",' ',df.loc[index,columnName])                       # remove unwanted text\n",
        "    df[columnName][index] = re.sub(r\"to:\",' ',df.loc[index,columnName])                         # remove unwanted text\n",
        "    df[columnName][index] = re.sub(r\"subject:\",' ',df.loc[index,columnName])                    # remove unwanted text  \n",
        "    df[columnName][index] = re.sub(r\"sent:\",' ',df.loc[index,columnName])                       # remove unwanted text\n",
        "    df[columnName][index] = re.sub(r\"ic:\",' ',df.loc[index,columnName])                         # remove unwanted text\n",
        "    df[columnName][index] = re.sub(r\"cc:\",' ',df.loc[index,columnName])                         # remove unwanted text\n",
        "    df[columnName][index] = re.sub(r\"bcc:\",' ',df.loc[index,columnName])                        # remove unwanted text  \n",
        "    df[columnName][index] = re.sub(r'\\d+','' ,df.loc[index,columnName])                         # remove numbers\n",
        "    df[columnName][index] = re.sub(r'\\n',' ',df.loc[index,columnName])                          # remove new line character\n",
        "    df[columnName][index] = re.sub(r'#','', df.loc[index,columnName])                           # remove hashtag while keeping hashtag text\n",
        "    df[columnName][index] = re.sub(r'&;?', 'and',df.loc[index,columnName])                      # remove &\n",
        "    df[columnName][index] = re.sub(r'\\&\\w*;', '', df.loc[index,columnName])                     # remove HTML special entities (e.g. &amp;)\n",
        "    df[columnName][index] = re.sub(r'https?:\\/\\/.*\\/\\w*', '', df.loc[index,columnName])         # remove hyperlinks\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VytpK4TzTgWw"
      },
      "source": [
        "df_clean = fn_remove_irrelaventWords(dataset1,'combined_description')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HYfpwSSeUCxF"
      },
      "source": [
        "df_clean.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xlzUhgXFTlPV"
      },
      "source": [
        "df_clean.to_csv(\"sample_data/dataset_clean_combined_description\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jf-DZpRiT9K9"
      },
      "source": [
        "df_clean.insert(loc=5,column='ConvertedToEnglish',value = np.nan, allow_duplicates = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJDvQnTTcyYB"
      },
      "source": [
        "svc_domains = ['.com','.com.au','.com.ar','.co.kr','.co.in','.co.jp','.at','.de','.ru','.ch','.fr','.es','.ae']\n",
        "svc_urls = ['http://translate.google' + domain for domain in svc_domains]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-OhHM6LWdCLw"
      },
      "source": [
        "trans_8416 = gs.translate(df_clean['combined_description'][8416], target_language='en', source_language='auto')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41Dj3EeQdEus"
      },
      "source": [
        "print ('Original Text : ',df_clean['combined_description'][8416])\n",
        "print('Traslated to English : ',trans_8416)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1w6hpVY3diE3"
      },
      "source": [
        "def fn_ConvertToEnglish(df,columnName):\n",
        "  for idx in range(df.shape[0]):\n",
        "    row_iter = gs.translate(df[columnName][idx],target_language='en',source_language = 'auto')\n",
        "    df[columnName][idx] = str(row_iter)\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fOD1IAXNdQBH"
      },
      "source": [
        "df_translated = fn_ConvertToEnglish(df_clean,'ConvertedToEnglish')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lo8kVUfRe0Nz"
      },
      "source": [
        "df_translated.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YaibaequhkzY"
      },
      "source": [
        "df_translated.to_csv('/sample_data/dataset_mojibaked_LangDetect_Translated.csv')')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UkwOcDsTh6b3"
      },
      "source": [
        "df_ML = df_translated.copy()\n",
        "df_DL = df_translated.copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AfRhk6YuiJUA"
      },
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JafekyPziMUR"
      },
      "source": [
        "stopwords = set(stopwords.words('english'))\n",
        " # Remove stopwords\n",
        "df_ML['ConvertedToEnglish'] = df_ML['ConvertedToEnglish'].apply(lambda x: ' '.join([word for word in x.split() if word not in stopwords]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtzAF0ssiT70"
      },
      "source": [
        "#lemmatization\n",
        "# Initialize spacy 'en' medium model, keeping only tagger component needed for lemmatization\n",
        "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
        "\n",
        "# Define a function to lemmatize the descriptions\n",
        "def lemmatizer(sentence):\n",
        "    # Parse the sentence using the loaded 'en' model object `nlp`\n",
        "    doc = nlp(sentence)\n",
        "    return \" \".join([token.lemma_ for token in doc if token.lemma_ !='-PRON-'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2c_F5BjPiaAK"
      },
      "source": [
        "print('\\033[1mOriginal text:\\033[0m')\n",
        "print(df_final['ConvertedToEnglish'][50])\n",
        "print('_'*100)\n",
        "print('\\033[1mLemmatized text:\\033[0m')\n",
        "print(lemmatizer(df_final['ConvertedToEnglish'][50]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2F6slFRib_i"
      },
      "source": [
        "df_ML['ConvertedToEnglish'] = df_ML['ConvertedToEnglish'].apply(lemmatizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LoKnt18gifZY"
      },
      "source": [
        "df_ML.to_csv(\"/sample_data/dataset_Cleaned_Translated_StopWords_Lemmatized.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWQluykvi3-A"
      },
      "source": [
        "df_ML.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0zmtYjuBj52v"
      },
      "source": [
        "df_ML.insert(loc=5,column='pred_group',value=np.nan,allow_duplicates=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2xmQPeNjj8t2"
      },
      "source": [
        "def deterministicRules(df,columnName):\n",
        "  for i in range(df.shape[0]):\n",
        "      #1 Contains telephony_software > GRP_7\n",
        "    if pd.notna(df[columnName][i]):\n",
        "      if ('telephonysoftware' in df[columnName][i]):\n",
        "        df['pred_group'][i] = 'GRP_7'\n",
        "      #2 contains cutview >  GRP_66\n",
        "      elif ('cutview' in df[columnName][i]):\n",
        "        df['pred_group'][i] = 'GRP_66'\n",
        "      #3 contains engg application >  GRP_58\n",
        "      elif ('engg application' in df[columnName][i]):\n",
        "        df['pred_group'][i] = 'GRP_58'\n",
        "      #4 contains ethics >  GRP_23\n",
        "      elif ('ethics' in df[columnName][i]):\n",
        "        df['pred_group'][i] = 'GRP_23'\n",
        "      # contains crm dynamics >  GRP_22\n",
        "      elif ('crm dynamics' in df[columnName][i]):\n",
        "        df['pred_group'][i] = 'GRP_22'\n",
        "      # contains distributor tool & company center >  GRP_21\n",
        "      elif ('distributor tool' in df[columnName][i]):\n",
        "        df['pred_group'][i] = 'GRP_21'\n",
        "      elif ('company center' in df[columnName][i]):\n",
        "        df['pred_group'][i] = 'GRP_21'\n",
        "      # contains bpctwhsn kzqsbmtp & network outage or circuit outage >  GRP_8\n",
        "      elif (df['Caller'][i] == 'bpctwhsn kzqsbmtp' and ('network outage' in df[columnName][i] or 'circuit outage' in df[columnName][i])):\n",
        "        df['pred_group'][i] = 'GRP_8'\n",
        "      elif ('reset passwords' in df[columnName][i] and 'the' in df[columnName][i]):\n",
        "        df['pred_group'][i] = 'GRP_17'\n",
        "      elif (df[columnName][i].startswith('erp access issue')):\n",
        "        df['pred_group'][i] = 'GRP_2'\n",
        "      elif ('vsphere' in df[columnName][i] or 'esxi' in df[columnName][i]):\n",
        "        df['pred_group'][i] = 'GRP_12'\n",
        "      elif ('windows account' in df[columnName][i]):\n",
        "        df['pred_group'][i] = 'GRP_0'\n",
        "      elif ('erp sid account lock' in df[columnName][i]):\n",
        "        df['pred_group'][i] = 'GRP_0'\n",
        "      elif ('erp sid password reset' in df[columnName][i]):\n",
        "        df['pred_group'][i] = 'GRP_0'\n",
        "      elif(df['Caller'][i] == 'jionmpsf wnkpzcmv' and 'eutool' in df[columnName][i]):\n",
        "        df['pred_group'][i] = 'GRP_24'\n",
        "      elif(df['Caller'][i] == 'cwrikael oanmsecr' and 'eutool' in df[columnName][i]):\n",
        "        df['pred_group'][i] = 'GRP_0'\n",
        "      elif ('sso portal' in df[columnName][i]):\n",
        "        df['pred_group'][i] = 'GRP_73'\n",
        "      elif ('unable complete forecast' in df[columnName][i]):\n",
        "        df['pred_group'][i] = 'GRP_67'\n",
        "      elif (df[columnName][i].startswith('timecard') or df[columnName][i].startswith('time card')):\n",
        "        df['pred_group'][i] = 'GRP_36'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i3zzv3N4j-2f"
      },
      "source": [
        "deterministicRules(df_ML,'ConvertedToEnglish')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gw-3VeSDkHY3"
      },
      "source": [
        "df_ML.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAHqRyWwkE64"
      },
      "source": [
        "df_determinted = df[~df['pred_group'].isna()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ld8FBYtNkKJ-"
      },
      "source": [
        "df_determinted.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ukeez4hvkMX-"
      },
      "source": [
        "df_determinted['pred_group'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}